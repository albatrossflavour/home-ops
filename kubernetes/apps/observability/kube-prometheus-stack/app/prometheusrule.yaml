---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/monitoring.coreos.com/prometheusrule_v1.json
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: miscellaneous-rules
spec:
  groups:
    - name: dockerhub
      rules:
        - alert: BootstrapRateLimitRisk
          annotations:
            summary: Kubernetes cluster at risk of being rate limited by dockerhub on bootstrap
          expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
          for: 15m
          labels:
            severity: critical
    - name: oom
      rules:
        - alert: OOMKilled
          annotations:
            summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
          expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
          labels:
            severity: critical
    - name: zfs
      rules:
        - alert: ZfsUnexpectedPoolState
          annotations:
            summary: ZFS pool {{$labels.zpool}} on {{$labels.instance}} is in a unexpected state {{$labels.state}}
          expr: node_zfs_zpool_state{state!="online"} > 0
          for: 15m
          labels:
            severity: critical
    - name: storage
      rules:
        - alert: PersistentVolumeClaimAlmostFull
          annotations:
            summary: PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full
            description: PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is at {{ $value | humanizePercentage }} capacity. Consider increasing size or cleaning up data.
          expr: |-
            (
              kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes
            ) > 0.75
          for: 5m
          labels:
            severity: warning
        - alert: PersistentVolumeClaimCriticallyFull
          annotations:
            summary: PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full
            description: PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is at {{ $value | humanizePercentage }} capacity. Immediate action required to prevent service disruption.
          expr: |-
            (
              kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes
            ) > 0.85
          for: 5m
          labels:
            severity: critical
        - alert: PersistentVolumeClaimFillingSoon
          annotations:
            summary: PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} will be full in 4 days
            description: Based on current usage trends, PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} will fill up in approximately 4 days.
          expr: |-
            (
              kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes
            ) < 0.15
            and
            predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0
          for: 1h
          labels:
            severity: warning
        - alert: CephClusterHealthError
          annotations:
            summary: Ceph cluster is in error state
            description: Ceph cluster health status is HEALTH_ERR. Check cluster status immediately.
          expr: ceph_health_status == 2
          for: 5m
          labels:
            severity: critical
        - alert: CephClusterHealthWarningSustained
          annotations:
            summary: Ceph cluster HEALTH_WARN persists for over 2 hours
            description: |
              Ceph cluster health status has been HEALTH_WARN for over 2 hours.
              This indicates a sustained issue beyond transient BlueStore compaction
              or hypervisor storage variance. Check 'ceph health detail' for root cause.
          expr: ceph_health_status == 1
          for: 2h
          labels:
            severity: warning
        - alert: CephDaemonSlowOps
          annotations:
            summary: Ceph daemon {{ $labels.ceph_daemon }} has {{ $value }} slow operations
            description: |
              Ceph daemon {{ $labels.ceph_daemon }} is reporting sustained slow operations.
              This may indicate performance issues beyond normal hypervisor storage variance.
              Check hypervisor storage backend and I/O contention.
          expr: ceph_daemon_health_metrics{type="SLOW_OPS"} > 50
          for: 30m
          labels:
            severity: warning
        - alert: CephOSDDown
          annotations:
            summary: Ceph OSD {{ $labels.ceph_daemon }} is down
            description: Ceph OSD {{ $labels.ceph_daemon }} has been down for more than 5 minutes.
          expr: ceph_osd_up == 0
          for: 5m
          labels:
            severity: critical
        - alert: VolsyncReplicationFailure
          annotations:
            summary: Volsync replication {{ $labels.namespace }}/{{ $labels.obj }} has failed
            description: Volsync replication {{ $labels.namespace }}/{{ $labels.obj }} last sync failed. Backups may be incomplete.
          expr: increase(volsync_missed_intervals_total[1h]) > 0
          for: 15m
          labels:
            severity: warning
        - alert: VolsyncReplicationStale
          annotations:
            summary: Volsync replication {{ $labels.namespace }}/{{ $labels.obj }} is stale
            description: Volsync replication {{ $labels.namespace }}/{{ $labels.obj }} has not completed successfully in over 48 hours.
          expr: (time() - volsync_last_sync_time_seconds) > (48 * 3600)
          for: 1h
          labels:
            severity: warning
    - name: target-monitoring
      rules:
        - alert: NodeExporterDown
          annotations:
            summary: Node exporter on {{ $labels.instance }} is unreachable
            description: Node exporter metrics from {{ $labels.instance }} have been unavailable for more than 5 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/targetdown
          expr: up{job="node-exporter",instance!~"(hex\\.home|dotty)\\.albatrossflavour\\.com.*"} == 0
          for: 5m
          labels:
            severity: warning
        - alert: NodeFilesystemSpaceFillingUp
          annotations:
            summary: Filesystem on {{ $labels.device }} at {{ $labels.instance }} is filling up
            description: |
              Filesystem on {{ $labels.device }} at {{ $labels.instance }} is predicted to fill up in less than 5 days.
              Current usage: {{ printf "%.2f" $value }}%
          expr: |
            (
              predict_linear(node_filesystem_avail_bytes{device=~"/dev/.*",fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat",instance!~"(hex\\.home|dotty)\\.albatrossflavour\\.com.*"}[6h], 5 * 24 * 3600) < 0
            and
              node_filesystem_avail_bytes{device=~"/dev/.*",fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat",instance!~"(hex\\.home|dotty)\\.albatrossflavour\\.com.*"} / node_filesystem_size_bytes{device=~"/dev/.*",fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat",instance!~"(hex\\.home|dotty)\\.albatrossflavour\\.com.*"} < 0.4
            and
              rate(node_filesystem_avail_bytes{device=~"/dev/.*",fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat",instance!~"(hex\\.home|dotty)\\.albatrossflavour\\.com.*"}[6h]) < 0
            )
            * on(instance, device) group_left(nodename)
            node_uname_info{instance!~"(hex\\.home|dotty)\\.albatrossflavour\\.com.*"}
          for: 1h
          labels:
            severity: warning
    - name: node-disk
      rules:
        - alert: NodeDiskIOSaturation
          annotations:
            description: |
              Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 20 for the last 30 minutes, is currently at {{ printf "%.2f" $value }}.
              This symptom might indicate disk saturation.
            summary: Disk IO queue is high.
          expr: rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)",instance!~"(hex\\.home|dotty)\\.albatrossflavour\\.com.*"}[5m]) > 20
          for: 30m
          labels:
            severity: warning
